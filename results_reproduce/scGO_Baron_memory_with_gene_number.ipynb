{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('data/baron/BaronMatrix.csv',index_col=0)\n",
    "annotation=pd.read_csv('data/baron/BaronMetaData.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation[\"celltype\"]=annotation[\"cell.type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"data/baron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20125\n",
      "7331\n"
     ]
    }
   ],
   "source": [
    "data_sparse=data\n",
    "\n",
    "\n",
    "#statistics of cells expressing each gene\n",
    "gene_expressed_cell_number=data_sparse.astype(bool).sum(axis=0)\n",
    "\n",
    "print(len(gene_expressed_cell_number))\n",
    "#filter gene expressed in less than 10 cells\n",
    "gene_expressed_cell_number=gene_expressed_cell_number[gene_expressed_cell_number>600]\n",
    "print(len(gene_expressed_cell_number))\n",
    "\n",
    "data_rm_sparse=data_sparse[gene_expressed_cell_number.index.tolist()]\n",
    "data_rm_sparse.shape           #10k cells, 4487 genes\n",
    "\n",
    "full_data=data_rm_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZNF821</th>\n",
       "      <th>PRPF40B</th>\n",
       "      <th>USF2</th>\n",
       "      <th>MTX2</th>\n",
       "      <th>CNOT3</th>\n",
       "      <th>UGCG</th>\n",
       "      <th>UBXN6</th>\n",
       "      <th>EDEM3</th>\n",
       "      <th>TJP2</th>\n",
       "      <th>POU5F2</th>\n",
       "      <th>...</th>\n",
       "      <th>U2AF1L4</th>\n",
       "      <th>COPS5</th>\n",
       "      <th>IGFBP5</th>\n",
       "      <th>RNF113A</th>\n",
       "      <th>YWHAH</th>\n",
       "      <th>IARS2</th>\n",
       "      <th>IL6R</th>\n",
       "      <th>ZCCHC2</th>\n",
       "      <th>C12orf75</th>\n",
       "      <th>ALG3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0002</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0003</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0004</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0005</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0697</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0698</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0699</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0700</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0701</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8569 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ZNF821  PRPF40B  USF2  MTX2  CNOT3  UGCG  UBXN6  \\\n",
       "human1_lib1.final_cell_0001       0        0     2     0      0     0      1   \n",
       "human1_lib1.final_cell_0002       2        0     1     1      0     1      1   \n",
       "human1_lib1.final_cell_0003       0        0     1     3      0     1      0   \n",
       "human1_lib1.final_cell_0004       0        0     3     1      0     0      0   \n",
       "human1_lib1.final_cell_0005       0        0     0     0      2     0      1   \n",
       "...                             ...      ...   ...   ...    ...   ...    ...   \n",
       "human4_lib3.final_cell_0697       0        0     0     0      0     1      1   \n",
       "human4_lib3.final_cell_0698       0        0     0     0      0     0      1   \n",
       "human4_lib3.final_cell_0699       0        0     0     0      0     0      0   \n",
       "human4_lib3.final_cell_0700       1        0     0     0      0     0      0   \n",
       "human4_lib3.final_cell_0701       0        0     0     0      0     0      1   \n",
       "\n",
       "                             EDEM3  TJP2  POU5F2  ...  U2AF1L4  COPS5  IGFBP5  \\\n",
       "human1_lib1.final_cell_0001      0     0       1  ...        0      1       0   \n",
       "human1_lib1.final_cell_0002      0     1       2  ...        0      1       0   \n",
       "human1_lib1.final_cell_0003      0     1       0  ...        0      0       0   \n",
       "human1_lib1.final_cell_0004      0     1       0  ...        0      1       0   \n",
       "human1_lib1.final_cell_0005      0     0       0  ...        0      1       0   \n",
       "...                            ...   ...     ...  ...      ...    ...     ...   \n",
       "human4_lib3.final_cell_0697      0     0       0  ...        0      0      23   \n",
       "human4_lib3.final_cell_0698      0     1       0  ...        0      2       0   \n",
       "human4_lib3.final_cell_0699      0     0       0  ...        0      0       0   \n",
       "human4_lib3.final_cell_0700      0     0       0  ...        0      0       0   \n",
       "human4_lib3.final_cell_0701      0     0       0  ...        0      0       0   \n",
       "\n",
       "                             RNF113A  YWHAH  IARS2  IL6R  ZCCHC2  C12orf75  \\\n",
       "human1_lib1.final_cell_0001        0      2      1     0       0         0   \n",
       "human1_lib1.final_cell_0002        0      1      0     0       0         0   \n",
       "human1_lib1.final_cell_0003        0      0      0     0       0         0   \n",
       "human1_lib1.final_cell_0004        0      4      0     0       0         1   \n",
       "human1_lib1.final_cell_0005        1      0      0     0       0         0   \n",
       "...                              ...    ...    ...   ...     ...       ...   \n",
       "human4_lib3.final_cell_0697        0      0      0     0       0         0   \n",
       "human4_lib3.final_cell_0698        0      0      0     1       0         1   \n",
       "human4_lib3.final_cell_0699        0      0      0     0       0         0   \n",
       "human4_lib3.final_cell_0700        0      0      0     0       0         1   \n",
       "human4_lib3.final_cell_0701        0      0      0     0       0         0   \n",
       "\n",
       "                             ALG3  \n",
       "human1_lib1.final_cell_0001     2  \n",
       "human1_lib1.final_cell_0002     4  \n",
       "human1_lib1.final_cell_0003     0  \n",
       "human1_lib1.final_cell_0004     0  \n",
       "human1_lib1.final_cell_0005     1  \n",
       "...                           ...  \n",
       "human4_lib3.final_cell_0697     0  \n",
       "human4_lib3.final_cell_0698     0  \n",
       "human4_lib3.final_cell_0699     0  \n",
       "human4_lib3.final_cell_0700     0  \n",
       "human4_lib3.final_cell_0701     0  \n",
       "\n",
       "[8569 rows x 500 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gene number = 200, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000\n",
    "import psutil\n",
    "\n",
    "\n",
    "memory_start=psutil.virtual_memory().used\n",
    "\n",
    "n=500\n",
    "\n",
    "data_rm_sparse=full_data.sample(axis=1,n=n)\n",
    "\n",
    "data_rm_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "len(TF_gene_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate gene_to_TF_transform_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "\n",
    "gene_number=len(data_rm_sparse.columns.to_list())    \n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "\n",
    "gene_to_TF_transform_matrix=np.zeros((gene_number,TF_number))\n",
    "\n",
    "TF_list=TF_gene_dict.keys()\n",
    "for i,gene in enumerate(data_rm_sparse.columns):\n",
    "    try:\n",
    "        j=TF_list.index(\"gene\")\n",
    "        gene_to_TF_transform_matrix[i][j]=1\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "gene_to_TF_transform_matrix\n",
    "\n",
    "pickle.dump(gene_to_TF_transform_matrix,open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393409\n",
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#generate TF_mask\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gene_TF_dict=pickle.load(open(\"human/gene_TF_dict\",\"rb\"))\n",
    "\n",
    "gene_number = len(data_rm_sparse.columns.to_list())    #6033\n",
    "TF_number = len(TF_gene_dict)\n",
    "\n",
    "TF_mask = np.zeros((gene_number,TF_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,TF in enumerate(TF_gene_dict):\n",
    "        if TF in gene_TF_dict.get(gene_id,[]):\n",
    "            TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "print(TF_mask)\n",
    "\n",
    "pickle.dump(TF_mask,open(\"%s/TF_mask\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946\n",
      "966768\n"
     ]
    }
   ],
   "source": [
    "#generate GO_mask\n",
    "\n",
    "GO_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"!\":\n",
    "            continue\n",
    "        \n",
    "        gene_id=line.split(\"\\t\")[2]\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if GO_term not in GO_dict:\n",
    "            GO_dict[GO_term]=[]\n",
    "        GO_dict[GO_term].append(gene_id)\n",
    "\n",
    "\n",
    "GO_list=[]\n",
    "count=0\n",
    "for item in GO_dict:\n",
    "    if len(GO_dict[item])>=30:\n",
    "        count+=1\n",
    "        GO_list.append(item)\n",
    "print(count)\n",
    "\n",
    "\n",
    "gene_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0]==\"!\":\n",
    "            continue\n",
    "        gene_id=line.split(\"\\t\")[2].upper()\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if gene_id not in gene_dict:\n",
    "            gene_dict[gene_id]=[]\n",
    "        gene_dict[gene_id].append(GO_term)\n",
    "\n",
    "\n",
    "\n",
    "gene_number=len(data_rm_sparse.columns)    #6033\n",
    "GO_number=len(GO_list)  \n",
    "\n",
    "GO_mask=np.zeros((gene_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,GO_term in enumerate(GO_list):\n",
    "        if GO_term in gene_dict.get(gene_id,\"GO:default\"):\n",
    "\n",
    "            GO_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "\n",
    "pickle.dump(GO_mask,open(\"%s/GO_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2332169\n"
     ]
    }
   ],
   "source": [
    "#generate GO_TF_mask\n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "GO_number=len(GO_list) \n",
    "\n",
    "GO_TF_mask=np.zeros((TF_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,TF in enumerate(TF_gene_dict):\n",
    "    for j,GO in enumerate(GO_list):\n",
    "        if GO in gene_dict.get(TF,\"GO:default\"):\n",
    "            GO_TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "print(error_count)\n",
    "        \n",
    "GO_TF_mask\n",
    "\n",
    "pickle.dump(GO_TF_mask,open(\"%s/GO_TF_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_multiclass_f1_score(true_labels, predicted_labels):\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    unique_labels = set(true_labels + predicted_labels)\n",
    "    f1_scores = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        true_positive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == label and pred == label)\n",
    "        false_positive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != label and pred == label)\n",
    "        false_negative = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == label and pred != label)\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory: 1335685120 25641496576\n",
      "fold 1-0 \taccuracy:\t 0.851808634772462 \tloss:\t 0.6818921240775482  \tf1 score:\t 0.48325156998028196\n",
      "memory: 1334550528 25640361984\n",
      "fold 1-1 \taccuracy:\t 0.9025670945157527 \tloss:\t 0.2709267908140369  \tf1 score:\t 0.6369497094368168\n",
      "memory: 1334808576 25640620032\n",
      "fold 1-2 \taccuracy:\t 0.926487747957993 \tloss:\t 0.1697746963928575  \tf1 score:\t 0.6709133863694383\n",
      "memory: 1285058560 25590870016\n",
      "fold 1-3 \taccuracy:\t 0.9165694282380397 \tloss:\t 0.12435587796828021  \tf1 score:\t 0.7172248030586023\n",
      "memory: 1278726144 25584537600\n",
      "fold 1-4 \taccuracy:\t 0.9189031505250875 \tloss:\t 0.07552665180486182  \tf1 score:\t 0.7373074382589081\n",
      "memory: 1282433024 25588244480\n",
      "fold 1-5 \taccuracy:\t 0.925904317386231 \tloss:\t 0.05607967447975407  \tf1 score:\t 0.7963198098665408\n",
      "memory: 1278717952 25584529408\n",
      "fold 1-6 \taccuracy:\t 0.9235705950991832 \tloss:\t 0.035605539450340944  \tf1 score:\t 0.7298341116384144\n",
      "memory: 1284288512 25590099968\n",
      "fold 1-7 \taccuracy:\t 0.9159859976662778 \tloss:\t 0.024038899811389652  \tf1 score:\t 0.7554782926234113\n",
      "memory: 1286123520 25591934976\n",
      "fold 1-8 \taccuracy:\t 0.9282380396732789 \tloss:\t 0.01699611890818114  \tf1 score:\t 0.7832522575822217\n",
      "memory: 1283280896 25589092352\n",
      "fold 1-9 \taccuracy:\t 0.927071178529755 \tloss:\t 0.00908555880634357  \tf1 score:\t 0.7614320658703967\n",
      "memory: 1283899392 25589710848\n",
      "fold 1-10 \taccuracy:\t 0.9276546091015169 \tloss:\t 0.005777073576641471  \tf1 score:\t 0.8212068167010941\n",
      "memory: 1285386240 25591197696\n",
      "fold 1-11 \taccuracy:\t 0.9235705950991832 \tloss:\t 0.003965827037894126  \tf1 score:\t 0.7934250487659606\n",
      "memory: 1288228864 25594040320\n",
      "fold 1-12 \taccuracy:\t 0.9224037339556592 \tloss:\t 0.0026638144948620997  \tf1 score:\t 0.7832703515360709\n",
      "memory: 1285586944 25591398400\n",
      "fold 1-13 \taccuracy:\t 0.9229871645274212 \tloss:\t 0.0019882007577942443  \tf1 score:\t 0.8131207845844215\n",
      "memory: 1285586944 25591398400\n",
      "fold 1-14 \taccuracy:\t 0.9229871645274212 \tloss:\t 0.0015770564384697734  \tf1 score:\t 0.7996697453855357\n",
      "memory: 1292795904 25598607360\n",
      "fold 2-0 \taccuracy:\t 0.9276546091015169 \tloss:\t 0.7221083104610443  \tf1 score:\t 0.4994435334026099\n",
      "memory: 1288282112 25594093568\n",
      "fold 2-1 \taccuracy:\t 0.9387397899649942 \tloss:\t 0.28701337462534077  \tf1 score:\t 0.5760811203435862\n",
      "memory: 1288429568 25594241024\n",
      "fold 2-2 \taccuracy:\t 0.9311551925320887 \tloss:\t 0.18208949844474379  \tf1 score:\t 0.596964160418782\n",
      "memory: 1291575296 25597386752\n",
      "fold 2-3 \taccuracy:\t 0.941073512252042 \tloss:\t 0.12602313816871333  \tf1 score:\t 0.6675292908703033\n",
      "memory: 1291866112 25597677568\n",
      "fold 2-4 \taccuracy:\t 0.9469078179696616 \tloss:\t 0.08543271115454643  \tf1 score:\t 0.7144636854536743\n",
      "memory: 1291452416 25597263872\n",
      "fold 2-5 \taccuracy:\t 0.9393232205367561 \tloss:\t 0.057868206784453084  \tf1 score:\t 0.621189439206373\n",
      "memory: 1290358784 25596170240\n",
      "fold 2-6 \taccuracy:\t 0.9463243873978997 \tloss:\t 0.03937812200544969  \tf1 score:\t 0.7044661015957232\n",
      "memory: 1285001216 25590812672\n",
      "fold 2-7 \taccuracy:\t 0.941656942823804 \tloss:\t 0.029803389122547663  \tf1 score:\t 0.683497054056039\n",
      "memory: 1291472896 25597284352\n",
      "fold 2-8 \taccuracy:\t 0.941656942823804 \tloss:\t 0.021510153254696532  \tf1 score:\t 0.6781911949321946\n",
      "memory: 1290457088 25596268544\n",
      "fold 2-9 \taccuracy:\t 0.9381563593932322 \tloss:\t 0.014961890700657893  \tf1 score:\t 0.6664582482734401\n",
      "memory: 1289297920 25595109376\n",
      "fold 2-10 \taccuracy:\t 0.9451575262543758 \tloss:\t 0.011748110573819798  \tf1 score:\t 0.718093677756873\n",
      "memory: 1289920512 25595731968\n",
      "fold 2-11 \taccuracy:\t 0.9439906651108518 \tloss:\t 0.006409261652784746  \tf1 score:\t 0.69915535645889\n",
      "memory: 1272827904 25578639360\n",
      "fold 2-12 \taccuracy:\t 0.9434072345390898 \tloss:\t 0.003947502481700529  \tf1 score:\t 0.7164690488726411\n",
      "memory: 1275412480 25581223936\n",
      "fold 2-13 \taccuracy:\t 0.9422403733955659 \tloss:\t 0.0027687106799535485  \tf1 score:\t 0.7123723688750435\n",
      "memory: 1273708544 25579520000\n",
      "fold 2-14 \taccuracy:\t 0.9422403733955659 \tloss:\t 0.001971357904688415  \tf1 score:\t 0.7117184694838617\n",
      "memory: 1280258048 25586069504\n",
      "fold 3-0 \taccuracy:\t 0.9282380396732789 \tloss:\t 0.6894337848476741  \tf1 score:\t 0.5574881236370504\n",
      "memory: 1275621376 25581432832\n",
      "fold 3-1 \taccuracy:\t 0.9422403733955659 \tloss:\t 0.2767227979133958  \tf1 score:\t 0.6441672203532565\n",
      "memory: 1281781760 25587593216\n",
      "fold 3-2 \taccuracy:\t 0.9364060676779463 \tloss:\t 0.16967268557004306  \tf1 score:\t 0.6298551181192432\n",
      "memory: 1283305472 25589116928\n",
      "fold 3-3 \taccuracy:\t 0.9369894982497082 \tloss:\t 0.12069117368563362  \tf1 score:\t 0.6516538282446586\n",
      "memory: 1281249280 25587060736\n",
      "fold 3-4 \taccuracy:\t 0.941656942823804 \tloss:\t 0.08393180719374314  \tf1 score:\t 0.6625191268189181\n",
      "memory: 1276301312 25582112768\n",
      "fold 3-5 \taccuracy:\t 0.9358226371061844 \tloss:\t 0.053403228707611564  \tf1 score:\t 0.5882215412455909\n",
      "memory: 1273511936 25579323392\n",
      "fold 3-6 \taccuracy:\t 0.9469078179696616 \tloss:\t 0.03527406518873961  \tf1 score:\t 0.6049769786996463\n",
      "memory: 1276301312 25582112768\n",
      "fold 3-7 \taccuracy:\t 0.94049008168028 \tloss:\t 0.03247622266897689  \tf1 score:\t 0.6021228263017278\n",
      "memory: 1280331776 25586143232\n",
      "fold 3-8 \taccuracy:\t 0.9469078179696616 \tloss:\t 0.017061999807154518  \tf1 score:\t 0.6050132094839278\n",
      "memory: 1273942016 25579753472\n",
      "fold 3-9 \taccuracy:\t 0.9463243873978997 \tloss:\t 0.013143795013994626  \tf1 score:\t 0.6058730792917039\n",
      "memory: 1276014592 25581826048\n",
      "fold 3-10 \taccuracy:\t 0.941656942823804 \tloss:\t 0.006855774919613791  \tf1 score:\t 0.5951073330148186\n",
      "memory: 1279954944 25585766400\n",
      "fold 3-11 \taccuracy:\t 0.9463243873978997 \tloss:\t 0.004152497672982028  \tf1 score:\t 0.6052964939933855\n",
      "memory: 1280786432 25586597888\n",
      "fold 3-12 \taccuracy:\t 0.9434072345390898 \tloss:\t 0.002906439029712878  \tf1 score:\t 0.5976724550316642\n",
      "memory: 1282150400 25587961856\n",
      "fold 3-13 \taccuracy:\t 0.9457409568261377 \tloss:\t 0.002172849939265491  \tf1 score:\t 0.5981718031533214\n",
      "memory: 1277771776 25583583232\n",
      "fold 3-14 \taccuracy:\t 0.9451575262543758 \tloss:\t 0.0017566986418703734  \tf1 score:\t 0.5991906931378653\n",
      "memory: 1276960768 25582772224\n",
      "fold 4-0 \taccuracy:\t 0.9072345390898483 \tloss:\t 0.7085405803245047  \tf1 score:\t 0.4870199877965798\n",
      "memory: 1277079552 25582891008\n",
      "fold 4-1 \taccuracy:\t 0.9235705950991832 \tloss:\t 0.2828708226590053  \tf1 score:\t 0.5918771676451551\n",
      "memory: 1279770624 25585582080\n",
      "fold 4-2 \taccuracy:\t 0.9352392065344224 \tloss:\t 0.1904001207779283  \tf1 score:\t 0.606078351200953\n",
      "memory: 1277644800 25583456256\n",
      "fold 4-3 \taccuracy:\t 0.9340723453908985 \tloss:\t 0.12978231988685288  \tf1 score:\t 0.7117807316004516\n",
      "memory: 1278926848 25584738304\n",
      "fold 4-4 \taccuracy:\t 0.9369894982497082 \tloss:\t 0.0904120278261278  \tf1 score:\t 0.6866298727978575\n",
      "memory: 1279873024 25585684480\n",
      "fold 4-5 \taccuracy:\t 0.9305717619603268 \tloss:\t 0.06177655869851942  \tf1 score:\t 0.6952206665769153\n",
      "memory: 1274462208 25580273664\n",
      "fold 4-6 \taccuracy:\t 0.9364060676779463 \tloss:\t 0.03925263450845428  \tf1 score:\t 0.7106198062638108\n",
      "memory: 1276534784 25582346240\n",
      "fold 4-7 \taccuracy:\t 0.9340723453908985 \tloss:\t 0.02597388990284146  \tf1 score:\t 0.7067625917487248\n",
      "memory: 1279565824 25585377280\n",
      "fold 4-8 \taccuracy:\t 0.9329054842473745 \tloss:\t 0.01781465687242377  \tf1 score:\t 0.7073568045928477\n",
      "memory: 1279827968 25585639424\n",
      "fold 4-9 \taccuracy:\t 0.9323220536756126 \tloss:\t 0.01237358017925821  \tf1 score:\t 0.7103300282025651\n",
      "memory: 1277444096 25583255552\n",
      "fold 4-10 \taccuracy:\t 0.9276546091015169 \tloss:\t 0.010029324200814186  \tf1 score:\t 0.7041746083181487\n",
      "memory: 1277427712 25583239168\n",
      "fold 4-11 \taccuracy:\t 0.926487747957993 \tloss:\t 0.033460900133840094  \tf1 score:\t 0.7004236462142841\n",
      "memory: 1279598592 25585410048\n",
      "fold 4-12 \taccuracy:\t 0.9200700116686115 \tloss:\t 0.007290571900691999  \tf1 score:\t 0.6995043813048144\n",
      "memory: 1283207168 25589018624\n",
      "fold 4-13 \taccuracy:\t 0.9311551925320887 \tloss:\t 0.003880434811277234  \tf1 score:\t 0.7019344597925735\n",
      "memory: 1279016960 25584828416\n",
      "fold 4-14 \taccuracy:\t 0.9288214702450408 \tloss:\t 0.002101594850714521  \tf1 score:\t 0.7044459029620881\n",
      "memory: 1287299072 25593110528\n",
      "fold 5-0 \taccuracy:\t 0.8493870402802102 \tloss:\t 0.6710250128870425  \tf1 score:\t 0.39424751961994214\n",
      "memory: 1283051520 25588862976\n",
      "fold 5-1 \taccuracy:\t 0.87215411558669 \tloss:\t 0.27314625366226486  \tf1 score:\t 0.4929607986324954\n",
      "memory: 1286197248 25592008704\n",
      "fold 5-2 \taccuracy:\t 0.9118505545826037 \tloss:\t 0.17546707325331543  \tf1 score:\t 0.6558719814917535\n",
      "memory: 1283026944 25588838400\n",
      "fold 5-3 \taccuracy:\t 0.8879159369527145 \tloss:\t 0.12339405040054217  \tf1 score:\t 0.6363994388149591\n",
      "memory: 1288110080 25593921536\n",
      "fold 5-4 \taccuracy:\t 0.8873321657910099 \tloss:\t 0.09842835565947967  \tf1 score:\t 0.6161310130757433\n",
      "memory: 1289089024 25594900480\n",
      "fold 5-5 \taccuracy:\t 0.8966725043782837 \tloss:\t 0.06110637751610383  \tf1 score:\t 0.5795592051724059\n",
      "memory: 1291608064 25597419520\n",
      "fold 5-6 \taccuracy:\t 0.882661996497373 \tloss:\t 0.04001975690784014  \tf1 score:\t 0.6479331411409751\n",
      "memory: 1292845056 25598656512\n",
      "fold 5-7 \taccuracy:\t 0.900758902510216 \tloss:\t 0.031152940636662685  \tf1 score:\t 0.6600924632232061\n",
      "memory: 1283137536 25588948992\n",
      "fold 5-8 \taccuracy:\t 0.8890834792761237 \tloss:\t 0.016108707055125546  \tf1 score:\t 0.5823613290923852\n",
      "memory: 1284993024 25590804480\n",
      "fold 5-9 \taccuracy:\t 0.9071803852889667 \tloss:\t 0.011810680540055846  \tf1 score:\t 0.6603689777542698\n",
      "memory: 1289297920 25595109376\n",
      "fold 5-10 \taccuracy:\t 0.8937536485697607 \tloss:\t 0.010889052292433284  \tf1 score:\t 0.5825328847406024\n",
      "memory: 1287548928 25593360384\n",
      "fold 5-11 \taccuracy:\t 0.890251021599533 \tloss:\t 0.009929551858120882  \tf1 score:\t 0.5800995741900735\n",
      "memory: 1287000064 25592811520\n",
      "fold 5-12 \taccuracy:\t 0.8949211908931699 \tloss:\t 0.0033433688556492006  \tf1 score:\t 0.6613173796979213\n",
      "memory: 1285480448 25591291904\n",
      "fold 5-13 \taccuracy:\t 0.8937536485697607 \tloss:\t 0.0022489318126842944  \tf1 score:\t 0.5844576827857545\n",
      "memory: 1283301376 25589112832\n",
      "fold 5-14 \taccuracy:\t 0.8937536485697607 \tloss:\t 0.001700942734580325  \tf1 score:\t 0.5857441387182705\n"
     ]
    }
   ],
   "source": [
    "#GO_Net\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split as ts\n",
    "\n",
    "data_rm_sparse=data_rm_sparse\n",
    "\n",
    "data_rm_sparse.index=annotation[\"celltype\"].to_list()\n",
    "\n",
    "###############################################################\n",
    "gene_to_TF_transform_matrix=pickle.load(open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"rb\"))\n",
    "TF_mask=pickle.load(open(\"%s/TF_mask\" %base_dir,\"rb\"))\n",
    "GO_mask=pickle.load(open(\"%s/GO_mask\" %base_dir,\"rb\"))\n",
    "GO_TF_mask=pickle.load(open(\"%s/GO_TF_mask\" %base_dir,\"rb\"))\n",
    "###############################################################\n",
    "#data_annotation = pd.read_csv('data/macparland/GSE115469_CellClusterType.txt', sep=\"\\t\")\n",
    "#index_rename_dict = {key: value for key, value in zip(data_annotation['CellName'], data_annotation['CellType'])}\n",
    "#$data_rm_sparse=data_rm_sparse.rename(index=index_rename_dict)\n",
    "\n",
    "#normalize by row\n",
    "#data_rm_sparse = data_rm_sparse.apply(lambda row: row / np.linalg.norm(row), axis=1)\n",
    "\n",
    "#merge similar cell types\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('Hepatocyte_\\d+', 'Hepatocyte', regex=True)\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('gamma-delta_T_Cells_\\d+', 'gamma-delta_T_Cells', regex=True)\n",
    "\n",
    "\n",
    "#filter low count cells\n",
    "#data_rm_sparse = data_rm_sparse[data_rm_sparse.index != 'Hepatic_Stellate_Cells']\n",
    "\n",
    "\n",
    "#novel_cell_type = ['Plasma_Cells']\n",
    "\n",
    "#data_rm_sparse_novel = data_rm_sparse[data_rm_sparse.index.isin(novel_cell_type)]\n",
    "#data_rm_sparse_rest = data_rm_sparse[~data_rm_sparse.index.isin(novel_cell_type)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes=[]\n",
    "for celltype in data_rm_sparse.index:\n",
    "    if celltype not in classes:\n",
    "        classes.append(celltype)\n",
    "#print(len(classes),classes)\n",
    "\n",
    "\n",
    "label_dict_revese={}\n",
    "label_dict={}\n",
    "for i,celltype in enumerate(classes):\n",
    "    label_dict[celltype]=i\n",
    "    label_dict_revese[i]=celltype\n",
    "label_dict\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def gen_mask(row,col,percent=0.5,num_zeros=None):\n",
    "    if num_zeros is None:\n",
    "        #Total number being masked is 0.5 by default\n",
    "        num_zeros=int((row*col)*percent)\n",
    "    \n",
    "    mask=np.hstack([np.zeros(num_zeros),np.ones(row*col-num_zeros)])\n",
    "    np.random.shuffle(mask)\n",
    "    return mask.reshape(row,col)\n",
    "\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    autograd function which masks it's weights by 'mask'.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Not that both forward and backword are @staticmethod\n",
    "\n",
    "    \n",
    "    #bias, mask is an optional argument\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None, mask=None):\n",
    "        if mask is not None:\n",
    "            #change weight to 0 where mask == 0\n",
    "\n",
    "            weight=weight*mask\n",
    " \n",
    "        output=input.mm(weight.t())\n",
    "\n",
    "        if bias is not None:\n",
    "            output+=bias.unsqueeze(0).expand_as(output)\n",
    "        \n",
    "        ctx.save_for_backward(input, weight, bias, mask)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    #This function has noly a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        input,weight,bias,mask = ctx.saved_tensors\n",
    "        grad_input=grad_weight=grad_bias=grad_mask=None\n",
    "        \n",
    "        #These meeds_input_grad checks are optional and there only to improve efficiency.\n",
    "        #If you want to make your code simpler, you can skip them. Returning gradients for\n",
    "        #inputs that don't require it is not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input=grad_output.mm(weight)\n",
    "        \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight=grad_output.t().mm(input)\n",
    "            \n",
    "            if mask is not None:\n",
    "                \n",
    "                #change grad_weight to 0 where mask == 0\n",
    "                grad_weight=grad_weight*mask\n",
    "\n",
    "        \n",
    "        #if bias is not None and ctx.need_input_grad[2]:\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_bias=grad_output.sum(0).squeeze(0)\n",
    "        \n",
    "        return grad_input,grad_weight,grad_bias,grad_mask\n",
    "    \n",
    "\n",
    "       \n",
    "class CustomizedLinear(nn.Module):\n",
    "    def __init__(self,input_features,output_features, bias=None, mask=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        mask [numpy array]:\n",
    "            The shape is (n_input_fearues,n_output_features).\n",
    "            The elements are 0 or 1 which delcare un-connected or connected.\n",
    "            \n",
    "        bias [bool]:\n",
    "            flg of bias.\n",
    "        \"\"\"\n",
    "        super(CustomizedLinear,self).__init__()\n",
    "        self.input_features=input_features\n",
    "        self.out_features=output_features\n",
    "        \n",
    "        #nn.Parameter is a spetial kind of Tensor, that will get\n",
    "        #automatically registered as Module's parameter once it's assigned\n",
    "        #as an attribute\n",
    "        self.weight=nn.Parameter(torch.Tensor(self.out_features,self.input_features))\n",
    "        \n",
    "        if bias:\n",
    "\n",
    "            self.bias=nn.Parameter(torch.Tensor(self.out_features))\n",
    "        else:\n",
    "            #You should always register all possible parameters, but the\n",
    "            #optinal ones can be None if you want.\n",
    "            self.register_parameter(\"bias\",None)\n",
    "            \n",
    "        #Initialize the above parameters (weight and bias). Important!\n",
    "        self.init_params()\n",
    "        \n",
    "        #mask should be registered after weight and bias\n",
    "        if mask is not None:\n",
    "            mask=torch.tensor(mask,dtype=torch.float).t()\n",
    "            self.mask=nn.Parameter(mask,requires_grad=False)\n",
    "        else:\n",
    "            self.register_parameter(\"mask\",None)\n",
    "\n",
    "        \n",
    "    def init_params(self):\n",
    "        stdv=1./math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv,stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv,stdv)\n",
    "                \n",
    "    def forward(self,input):\n",
    "        #See the autograd section for explanation of what happens here.\n",
    "        \n",
    "        output=LinearFunction.apply(input,self.weight,self.bias,self.mask)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        #(Optional) Set the extra informatioin about this module. You can test\n",
    "        #it by printing an object of this class.\n",
    "        return \"input_features={}, output_features={}, bias={}, mask={}\".format(\n",
    "            self.input_features, self.out_features,\n",
    "            self.bias is not None, self.mask is not None)\n",
    "        \n",
    "        \n",
    "class GO_Net(nn.Module):\n",
    "    def __init__(self,in_size,out_size,ratio=[0.006525,0,0]):\n",
    "        super(GO_Net,self).__init__()\n",
    "\n",
    "        self.gene_number=len(data_rm_sparse.columns)    #6033\n",
    "        self.TF_number=1209\n",
    "        self.GO_number=len(GO_list)\n",
    "        self.class_number=3\n",
    "\n",
    "        self.gene_to_TF_transform_matrix=torch.tensor(gene_to_TF_transform_matrix,dtype=torch.float32)\n",
    "    \n",
    "        \n",
    "        self.bn0=nn.BatchNorm1d(self.gene_number)\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=gen_mask(3443,2290,ratio[0]))  \n",
    "        #self.fc1=CustomizedLinear(in_size,1946,mask=gen_mask(2903,1946,ratio[0]))        \n",
    "        self.fc1=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        self.gene_to_GO_layer=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=np.ones((3443,2290)))\n",
    "    \n",
    "        self.bn1=nn.BatchNorm1d(self.GO_number)\n",
    "                \n",
    "        self.fc2=CustomizedLinear(self.GO_number,out_size,mask=gen_mask(self.GO_number,out_size,ratio[1]))\n",
    "        self.bn2=nn.BatchNorm1d(out_size)\n",
    "\n",
    "        self.gene_to_TF_layer=CustomizedLinear(self.gene_number,self.TF_number,mask=TF_mask)\n",
    "        self.TF_to_GO_layer=CustomizedLinear(self.TF_number,self.GO_number,mask=GO_TF_mask)\n",
    "        \n",
    "        self.fc3=CustomizedLinear(100,100,mask=gen_mask(100,100,ratio[1]))\n",
    "\n",
    "        self.fc4=CustomizedLinear(100,out_size,mask=gen_mask(100,out_size,ratio[1]))\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        self.leaky_relu=nn.LeakyReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.uniform_(module.weight,a=0,b=1)\n",
    "            elif isinstance(module,(nn.BatchNorm1d,nn.GroupNorm)):\n",
    "                nn.init.constant_(module.weight,1)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "                        \n",
    "    def forward(self,x):\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        TF_residul=torch.matmul(x,self.gene_to_TF_transform_matrix)\n",
    "\n",
    "        TF_derived_from_gene=self.gene_to_TF_layer(x)\n",
    "\n",
    "        TF_sum=TF_residul+TF_derived_from_gene\n",
    "        #TF_sum=TF_derived_from_gene\n",
    "\n",
    "        GO_derived_from_TF=self.TF_to_GO_layer(TF_sum)\n",
    "\n",
    "        GO_derived_from_gene=self.gene_to_GO_layer(x)\n",
    "\n",
    "        GO_sum=GO_derived_from_TF+GO_derived_from_gene\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        #x=self.fc1(x)\n",
    "        #x=self.bn1(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.dropout(x)\n",
    "        GO_sum=self.leaky_relu(GO_sum)\n",
    "\n",
    "        #x=torch.tanh(x) \n",
    "        #print(161,self.fc1.weight)\n",
    "        x=self.fc2(GO_sum)\n",
    "        #x=self.bn2(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc3(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc4(x)\n",
    " \n",
    "        return x,GO_sum,TF_derived_from_gene,GO_derived_from_TF\n",
    "\n",
    "\"\"\"\n",
    "class Reconstraction(nn.Module):\n",
    "    def __init__(self,in_size,out_size):\n",
    "        super(Reconstraction,self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_size, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 1000),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(1000, out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\"\"\"     \n",
    "\n",
    "\n",
    " \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.x[index]\n",
    "        label = self.y[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def accuracy_score(y_test,y_pred):\n",
    "    t=0\n",
    "    f=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==y_pred[i]:\n",
    "            t+=1\n",
    "        else:\n",
    "            f+=1\n",
    "    return(t/(t+f))\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def make_weights_for_balanced_classes(dataset, nclasses):\n",
    "    count = [0] * nclasses\n",
    "    for item in dataset:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    N = float(sum(count))\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(dataset)\n",
    "    for idx, val in enumerate(dataset):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight\n",
    "\n",
    "\n",
    "class CustomWeightedRandomSampler(WeightedRandomSampler):\n",
    "    \"\"\"WeightedRandomSampler except allows for more than 2^24 samples to be sampled\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_tensor = np.random.choice(range(0, len(self.weights)),\n",
    "                                       size=self.num_samples,\n",
    "                                       p=self.weights.numpy() / torch.sum(self.weights).numpy(),\n",
    "                                       replace=self.replacement)\n",
    "        rand_tensor = torch.from_numpy(rand_tensor)\n",
    "        return iter(rand_tensor.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#training\n",
    "input_size = len(data_rm_sparse.columns)\n",
    "output_size = len(classes)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "#reconstraction_optimizer = optim.Adam(reconstraction_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#reconstraction_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "data_train_x=data_rm_sparse\n",
    "data_train_y=data_rm_sparse.index\n",
    "\n",
    "\n",
    "#5-fold cross validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Generate 5-fold cross-validation indices\n",
    "kf = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_indices = list(kf.split(data))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for fold, (train_indices, test_indices) in enumerate(fold_indices, start=1):\n",
    "\n",
    "    #define model and optimizer\n",
    "    model = GO_Net(input_size, output_size,ratio=[0,0,0])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # Subset data and annotation based on indices\n",
    "    x_train = data_train_x.iloc[train_indices].to_numpy()\n",
    "    y_train = data_train_y[train_indices,]\n",
    "    \n",
    "    x_test = data_train_x.iloc[test_indices].to_numpy()\n",
    "    y_test = data_train_y[test_indices,]\n",
    "\n",
    "    # Continue with your operations on data_train, anno_train, data_test, and anno_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #x_train,x_test,y_train,y_test = ts(data_train_x.to_numpy(),data_train_y.to_numpy(),test_size=0.2,random_state=1, shuffle=True)\n",
    "\n",
    "    #x_train=x_train[0:400]\n",
    "    #y_train=y_train[0:400]\n",
    "\n",
    "    #label_dict={25:0,26:1,27:2,33:3,34:4}\n",
    "    y_train_relabeled=[label_dict[label] for label in y_train]\n",
    "    y_test_relabeled=[label_dict[label] for label in y_test]\n",
    "\n",
    "\n",
    "    #train_size=20000\n",
    "\n",
    "    #x_train=x_train[0:train_size]\n",
    "    #y_train_relabeled=y_train_relabeled[0:train_size]\n",
    "\n",
    "    train_data=MyDataset(x_train,y_train_relabeled)\n",
    "\n",
    "\n",
    "\n",
    "    #for unbalanced data\n",
    "    \"\"\"\n",
    "    weights=make_weights_for_balanced_classes(train_data,len(classes))\n",
    "    weights = torch.DoubleTensor(weights)\n",
    "    sampler = CustomWeightedRandomSampler(weights, len(weights))        #sampler for imbalanced classes\n",
    "    \"\"\"\n",
    "\n",
    "    #train_loader=DataLoader(train_data, batch_size=60, sampler=sampler)\n",
    "    train_loader=DataLoader(train_data, batch_size=60, shuffle=True)\n",
    "\n",
    "    num_epochs=15\n",
    "    # è®­ç»ƒæ¨¡åž‹\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        reconstraction_running_loss = 0.0\n",
    "\n",
    "        \n",
    "\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            inputs, labels = batch\n",
    "            #print(labels)\n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            # å°†æ¢¯åº¦ç¼“å­˜æ¸…é›¶\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # å‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±å’Œåå‘ä¼ æ’­\n",
    "            outputs,_,_,_ = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #reconstraction_input=reconstraction_model(outputs)\n",
    "            #reconstraction_loss = reconstraction_criterion(reconstraction_input, inputs)\n",
    "\n",
    "            #reconstraction_optimizer.zero_grad()\n",
    "\n",
    "            #combined_loss=loss+reconstraction_loss\n",
    "            #combined_loss.backward()\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            #reconstraction_optimizer.step()\n",
    "\n",
    "\n",
    "            #reconstraction_running_loss += reconstraction_loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 40 == 0:\n",
    "                pass\n",
    "                #print(i)\n",
    "                #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            \n",
    "            if i>400:\n",
    "                break\n",
    "\n",
    "        test_data=MyDataset(x_test,y_test_relabeled)\n",
    "        test_loader=DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "        result=[]\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            \n",
    "            outputs,_,_,_ = model(inputs)\n",
    "            pred = list(torch.max(outputs, 1)[1].numpy())\n",
    "            result.extend(pred)\n",
    "            #print(pred,labels)\n",
    "            if i>100:\n",
    "                break\n",
    "        accuracy = accuracy_score(y_test_relabeled[0:len(result)],result)\n",
    "        f1_score = calculate_multiclass_f1_score(y_test_relabeled[0:len(result)],result)\n",
    "                #########\n",
    "        memory_end=psutil.virtual_memory().used\n",
    "        print(\"memory:\",memory_end-memory_start,memory_end)\n",
    "        \n",
    "        print(\"fold %s-%s\" %(fold,epoch),\"\\taccuracy:\\t\",accuracy,\"\\tloss:\\t\",running_loss / len(train_loader),\" \\tf1 score:\\t\",f1_score )\n",
    "        \n",
    "        #save model\n",
    "        #pickle.dump(model,open(\"model/GO_heart.model\",\"wb\"))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Probabilities  Proportion\n",
      "0           100      25.636\n",
      "1           200      25.619\n",
      "2           500      25.591\n",
      "3          1000      25.609\n",
      "4          2000      25.568\n",
      "5          3000      25.595\n",
      "6          4000      25.832\n",
      "7          5000      26.043\n",
      "8          6000      26.355\n",
      "9          7000      26.488\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 354x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 180,
       "width": 354
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/.conda/envs/sc/lib/python3.8/site-packages/plotnine/ggplot.py:587: PlotnineWarning: Saving 3.54 x 1.8 in image.\n",
      "/home/wuyou/.conda/envs/sc/lib/python3.8/site-packages/plotnine/ggplot.py:588: PlotnineWarning: Filename: figure/memory_usage_with_gene_number.pdf\n"
     ]
    }
   ],
   "source": [
    "#probabilities cutoff barplot\n",
    "X=[\"100\",\"200\",\"500\",\"1000\",\"2000\",\"3000\",\"4000\",\"5000\",\"6000\",\"7000\"]\n",
    "Y=[25.636, 25.619, 25.591,25.609, 25.568, 25.595, 25.832, 26.043, 26.355, 26.488]\n",
    "\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "\n",
    "black = '#222222'\n",
    "gray = '#666666'\n",
    "red = '#FF3333'\n",
    "green = '#66CC00'\n",
    "blue = '#3333FF'\n",
    "purple = '#9933FF'\n",
    "orange = '#FF8000'\n",
    "yellow = '#FFFF33'\n",
    "c1=\"#F8766D\"\n",
    "c2=\"#00BA38\" \n",
    "c3=\"#619CFF\"\n",
    "\n",
    "data=pd.DataFrame(dict(Probabilities=X,Proportion=Y)) \n",
    "data['Probabilities'] = pd.Categorical(data.Probabilities, categories=pd.unique(data.Probabilities))  #reorder legend\n",
    "\n",
    "print(data)\n",
    "p1 = (ggplot()\n",
    "        +geom_bar(data,aes(x=\"Probabilities\",y = \"Proportion\"),stat=\"identity\",width=0.6,fill=c1,alpha=0.8)\n",
    "        \n",
    "        +theme(panel_background=element_rect(fill=gray, alpha=0),\n",
    "            panel_grid_major=element_line(size=0.3, alpha=0,color=black),\n",
    "            panel_grid_minor=element_line(size=0.3, alpha=0,color=black),\n",
    "            panel_border=element_rect(color=black, size=1),\n",
    "            axis_text=element_text(size=6,family=\"Arial\",color=\"black\"),\n",
    "            axis_title_x=element_text(size=6,family=\"Arial\",color=\"black\"),\n",
    "            axis_title_y=element_text(size=6,family=\"Arial\",color=\"black\"),\n",
    "            axis_text_x=element_text(rotation=0, hjust=0.5),\n",
    "            figure_size=[3.54,1.8],\n",
    "            legend_title = element_text(size=6), #change legend title font size\n",
    "            legend_text = element_text(size=6),\n",
    "            legend_background=element_rect(size=0.5,alpha=0),\n",
    "            legend_position=(0.60,0.4),\n",
    "            legend_key_size=4) #change legend text font size\n",
    "        \n",
    "\n",
    "        +labs(x = \"Input feature size (gene number)\", y =\"Memory usage (GB)\")\n",
    "        +guides(color = guide_legend(title = \"Probability cutoff\"))\n",
    "        #+ylim(10,30)   # not work.\n",
    "        + coord_cartesian(ylim=(20, 27))\n",
    "        \n",
    ")\n",
    "print(p1)\n",
    "p1.save('figure/memory_usage_with_gene_number.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('data/baron/BaronMatrix.csv',index_col=0)\n",
    "annotation=pd.read_csv('data/baron/BaronMetaData.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation[\"celltype\"]=annotation[\"cell.type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"data/baron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20125\n",
      "7331\n"
     ]
    }
   ],
   "source": [
    "data_sparse=data\n",
    "\n",
    "\n",
    "#statistics of cells expressing each gene\n",
    "gene_expressed_cell_number=data_sparse.astype(bool).sum(axis=0)\n",
    "\n",
    "print(len(gene_expressed_cell_number))\n",
    "#filter gene expressed in less than 10 cells\n",
    "gene_expressed_cell_number=gene_expressed_cell_number[gene_expressed_cell_number>600]\n",
    "print(len(gene_expressed_cell_number))\n",
    "\n",
    "data_rm_sparse=data_sparse[gene_expressed_cell_number.index.tolist()]\n",
    "data_rm_sparse.shape           #10k cells, 4487 genes\n",
    "\n",
    "full_data=data_rm_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIP1</th>\n",
       "      <th>HNRNPH1</th>\n",
       "      <th>LAMC2</th>\n",
       "      <th>LIPA</th>\n",
       "      <th>PTBP1</th>\n",
       "      <th>CMTM8</th>\n",
       "      <th>RNF187</th>\n",
       "      <th>MRPS18B</th>\n",
       "      <th>HK1</th>\n",
       "      <th>MED28</th>\n",
       "      <th>...</th>\n",
       "      <th>SYNE4</th>\n",
       "      <th>MESDC2</th>\n",
       "      <th>BAG1</th>\n",
       "      <th>TSC22D4</th>\n",
       "      <th>SURF1</th>\n",
       "      <th>MRPS27</th>\n",
       "      <th>DUSP11</th>\n",
       "      <th>MARCH6</th>\n",
       "      <th>CSNK1G3</th>\n",
       "      <th>FAM122B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0001</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0002</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0003</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0004</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human1_lib1.final_cell_0005</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0697</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0698</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0699</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0700</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human4_lib3.final_cell_0701</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8569 rows Ã— 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CLIP1  HNRNPH1  LAMC2  LIPA  PTBP1  CMTM8  \\\n",
       "human1_lib1.final_cell_0001      1        0      0     1      2      1   \n",
       "human1_lib1.final_cell_0002      2        2      0     0      8      0   \n",
       "human1_lib1.final_cell_0003      2        0      0     0      1      2   \n",
       "human1_lib1.final_cell_0004      0        0      0     0      0      1   \n",
       "human1_lib1.final_cell_0005      0        5      0     0      1      0   \n",
       "...                            ...      ...    ...   ...    ...    ...   \n",
       "human4_lib3.final_cell_0697      0        0      0     0      0      0   \n",
       "human4_lib3.final_cell_0698      0        0      0     0      1      0   \n",
       "human4_lib3.final_cell_0699      0        0      0     0      0      0   \n",
       "human4_lib3.final_cell_0700      0        0      0     0      0      0   \n",
       "human4_lib3.final_cell_0701      0        2      0     0      0      1   \n",
       "\n",
       "                             RNF187  MRPS18B  HK1  MED28  ...  SYNE4  MESDC2  \\\n",
       "human1_lib1.final_cell_0001       6        0    0      1  ...      0       3   \n",
       "human1_lib1.final_cell_0002       2        1    0      2  ...      1       1   \n",
       "human1_lib1.final_cell_0003       1        0    0      1  ...      0       2   \n",
       "human1_lib1.final_cell_0004       2        4    0      0  ...      0       1   \n",
       "human1_lib1.final_cell_0005       1        1    0      0  ...      0       0   \n",
       "...                             ...      ...  ...    ...  ...    ...     ...   \n",
       "human4_lib3.final_cell_0697       0        0    0      0  ...      0       0   \n",
       "human4_lib3.final_cell_0698       0        0    0      0  ...      0       0   \n",
       "human4_lib3.final_cell_0699       0        0    0      0  ...      0       0   \n",
       "human4_lib3.final_cell_0700       1        0    0      0  ...      0       1   \n",
       "human4_lib3.final_cell_0701       0        0    0      0  ...      0       1   \n",
       "\n",
       "                             BAG1  TSC22D4  SURF1  MRPS27  DUSP11  MARCH6  \\\n",
       "human1_lib1.final_cell_0001     3        0      1       1       0       3   \n",
       "human1_lib1.final_cell_0002     1        0      1       1       0       5   \n",
       "human1_lib1.final_cell_0003     0        0      0       0       0       0   \n",
       "human1_lib1.final_cell_0004     3        0      1       1       0       0   \n",
       "human1_lib1.final_cell_0005     1        0      0       0       0       0   \n",
       "...                           ...      ...    ...     ...     ...     ...   \n",
       "human4_lib3.final_cell_0697     0        0      0       0       0       0   \n",
       "human4_lib3.final_cell_0698     0        0      1       0       0       0   \n",
       "human4_lib3.final_cell_0699     1        0      0       0       0       1   \n",
       "human4_lib3.final_cell_0700     0        0      0       0       0       0   \n",
       "human4_lib3.final_cell_0701     1        0      0       0       0       0   \n",
       "\n",
       "                             CSNK1G3  FAM122B  \n",
       "human1_lib1.final_cell_0001        0        0  \n",
       "human1_lib1.final_cell_0002        1        0  \n",
       "human1_lib1.final_cell_0003        0        0  \n",
       "human1_lib1.final_cell_0004        1        0  \n",
       "human1_lib1.final_cell_0005        0        0  \n",
       "...                              ...      ...  \n",
       "human4_lib3.final_cell_0697        0        0  \n",
       "human4_lib3.final_cell_0698        0        0  \n",
       "human4_lib3.final_cell_0699        0        0  \n",
       "human4_lib3.final_cell_0700        0        0  \n",
       "human4_lib3.final_cell_0701        0        0  \n",
       "\n",
       "[8569 rows x 2000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gene number = 200, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000\n",
    "\n",
    "n=2000\n",
    "\n",
    "data_rm_sparse=full_data.sample(axis=1,n=n)\n",
    "\n",
    "data_rm_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "len(TF_gene_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate gene_to_TF_transform_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "\n",
    "gene_number=len(data_rm_sparse.columns.to_list())    \n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "\n",
    "gene_to_TF_transform_matrix=np.zeros((gene_number,TF_number))\n",
    "\n",
    "TF_list=TF_gene_dict.keys()\n",
    "for i,gene in enumerate(data_rm_sparse.columns):\n",
    "    try:\n",
    "        j=TF_list.index(\"gene\")\n",
    "        gene_to_TF_transform_matrix[i][j]=1\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "gene_to_TF_transform_matrix\n",
    "\n",
    "pickle.dump(gene_to_TF_transform_matrix,open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583209\n",
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#generate TF_mask\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gene_TF_dict=pickle.load(open(\"human/gene_TF_dict\",\"rb\"))\n",
    "\n",
    "gene_number = len(data_rm_sparse.columns.to_list())    #6033\n",
    "TF_number = len(TF_gene_dict)\n",
    "\n",
    "TF_mask = np.zeros((gene_number,TF_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,TF in enumerate(TF_gene_dict):\n",
    "        if TF in gene_TF_dict.get(gene_id,[]):\n",
    "            TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "print(TF_mask)\n",
    "\n",
    "pickle.dump(TF_mask,open(\"%s/TF_mask\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946\n",
      "Total GO number: 1946\n",
      "2383892\n"
     ]
    }
   ],
   "source": [
    "#generate GO_mask\n",
    "\n",
    "GO_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"!\":\n",
    "            continue\n",
    "        \n",
    "        gene_id=line.split(\"\\t\")[2]\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if GO_term not in GO_dict:\n",
    "            GO_dict[GO_term]=[]\n",
    "        GO_dict[GO_term].append(gene_id)\n",
    "\n",
    "\n",
    "GO_list=[]\n",
    "count=0\n",
    "for item in GO_dict:\n",
    "    if len(GO_dict[item])>=30:\n",
    "        count+=1\n",
    "        GO_list.append(item)\n",
    "print(count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total GO number:\",len(GO_list))\n",
    "\n",
    "import random\n",
    "\n",
    "n=1200\n",
    "\n",
    "GO_list = random.sample(GO_list, n)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gene_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0]==\"!\":\n",
    "            continue\n",
    "        gene_id=line.split(\"\\t\")[2].upper()\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if gene_id not in gene_dict:\n",
    "            gene_dict[gene_id]=[]\n",
    "        gene_dict[gene_id].append(GO_term)\n",
    "\n",
    "\n",
    "\n",
    "gene_number=len(data_rm_sparse.columns)    #6033\n",
    "GO_number=len(GO_list)  \n",
    "\n",
    "GO_mask=np.zeros((gene_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,GO_term in enumerate(GO_list):\n",
    "        if GO_term in gene_dict.get(gene_id,\"GO:default\"):\n",
    "\n",
    "            GO_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "\n",
    "pickle.dump(GO_mask,open(\"%s/GO_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437424\n"
     ]
    }
   ],
   "source": [
    "#generate GO_TF_mask\n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "GO_number=len(GO_list) \n",
    "\n",
    "GO_TF_mask=np.zeros((TF_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,TF in enumerate(TF_gene_dict):\n",
    "    for j,GO in enumerate(GO_list):\n",
    "        if GO in gene_dict.get(TF,\"GO:default\"):\n",
    "            GO_TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "print(error_count)\n",
    "        \n",
    "GO_TF_mask\n",
    "\n",
    "pickle.dump(GO_TF_mask,open(\"%s/GO_TF_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_multiclass_f1_score(true_labels, predicted_labels):\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    unique_labels = set(true_labels + predicted_labels)\n",
    "    f1_scores = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        true_positive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == label and pred == label)\n",
    "        false_positive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != label and pred == label)\n",
    "        false_negative = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == label and pred != label)\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1-0 \taccuracy:\t 0.9655775962660443 \tloss:\t 0.4752545800222003  \tf1 score:\t 0.7121121242264601 \ttime:\t 3.2505300045013428\n",
      "fold 1-1 \taccuracy:\t 0.969661610268378 \tloss:\t 0.12107541868868081  \tf1 score:\t 0.7124826934400156 \ttime:\t 2.9982402324676514\n",
      "fold 1-2 \taccuracy:\t 0.970828471411902 \tloss:\t 0.04143943154131589  \tf1 score:\t 0.8052078464816935 \ttime:\t 2.8081462383270264\n",
      "fold 1-3 \taccuracy:\t 0.9731621936989499 \tloss:\t 0.02000350160777326  \tf1 score:\t 0.8184638084532623 \ttime:\t 2.9415156841278076\n",
      "fold 1-4 \taccuracy:\t 0.9719953325554259 \tloss:\t 0.008412920168864177  \tf1 score:\t 0.7828067085364379 \ttime:\t 2.8799843788146973\n",
      "fold 1-5 \taccuracy:\t 0.970828471411902 \tloss:\t 0.0026247106202746456  \tf1 score:\t 0.8857832137955842 \ttime:\t 2.7899904251098633\n",
      "fold 1-6 \taccuracy:\t 0.969661610268378 \tloss:\t 0.0011005471234966536  \tf1 score:\t 0.8570477320810427 \ttime:\t 2.881192445755005\n",
      "fold 1-7 \taccuracy:\t 0.9714119019836639 \tloss:\t 0.0007166557599861255  \tf1 score:\t 0.7960363664254261 \ttime:\t 2.8604001998901367\n",
      "fold 1-8 \taccuracy:\t 0.9690781796966161 \tloss:\t 0.0005061679275493825  \tf1 score:\t 0.8717190402709146 \ttime:\t 2.9085378646850586\n",
      "fold 1-9 \taccuracy:\t 0.9702450408401401 \tloss:\t 0.0003792444237067526  \tf1 score:\t 0.8726953000444105 \ttime:\t 2.884629011154175\n",
      "fold 1-10 \taccuracy:\t 0.9679113185530922 \tloss:\t 0.00029450948600044836  \tf1 score:\t 0.8706619838031534 \ttime:\t 2.5839555263519287\n",
      "fold 1-11 \taccuracy:\t 0.9702450408401401 \tloss:\t 0.00023207632076936653  \tf1 score:\t 0.8732156918628381 \ttime:\t 2.4674036502838135\n",
      "fold 1-12 \taccuracy:\t 0.9702450408401401 \tloss:\t 0.0001851418863620695  \tf1 score:\t 0.8733085374559272 \ttime:\t 2.4804532527923584\n",
      "fold 1-13 \taccuracy:\t 0.9690781796966161 \tloss:\t 0.00015154229610846206  \tf1 score:\t 0.8716396570247311 \ttime:\t 2.5509798526763916\n",
      "fold 1-14 \taccuracy:\t 0.970828471411902 \tloss:\t 0.0001279087148878202  \tf1 score:\t 0.8740147078889318 \ttime:\t 2.672482490539551\n",
      "fold 2-0 \taccuracy:\t 0.9428238039673279 \tloss:\t 0.49507354860720426  \tf1 score:\t 0.6391470673497464 \ttime:\t 3.094773054122925\n",
      "fold 2-1 \taccuracy:\t 0.9749124854142357 \tloss:\t 0.11183952583890894  \tf1 score:\t 0.6897980291699513 \ttime:\t 2.817636489868164\n",
      "fold 2-2 \taccuracy:\t 0.9766627771295215 \tloss:\t 0.04890839115149625  \tf1 score:\t 0.7197717521564851 \ttime:\t 2.7602362632751465\n",
      "fold 2-3 \taccuracy:\t 0.9737456242707118 \tloss:\t 0.01869231995964504  \tf1 score:\t 0.8414349175020978 \ttime:\t 2.8178210258483887\n",
      "fold 2-4 \taccuracy:\t 0.9801633605600933 \tloss:\t 0.010362800705890707  \tf1 score:\t 0.7763897564982881 \ttime:\t 2.796790838241577\n",
      "fold 2-5 \taccuracy:\t 0.9801633605600933 \tloss:\t 0.018375416787887882  \tf1 score:\t 0.7587919992208504 \ttime:\t 2.7951319217681885\n",
      "fold 2-6 \taccuracy:\t 0.9784130688448075 \tloss:\t 0.006043814559613922  \tf1 score:\t 0.7446960558998872 \ttime:\t 2.842921733856201\n",
      "fold 2-7 \taccuracy:\t 0.9801633605600933 \tloss:\t 0.003519863414555363  \tf1 score:\t 0.7570283074005157 \ttime:\t 2.728614568710327\n",
      "fold 2-8 \taccuracy:\t 0.9784130688448075 \tloss:\t 0.0008042130408601066  \tf1 score:\t 0.7295525460755976 \ttime:\t 2.866400718688965\n",
      "fold 2-9 \taccuracy:\t 0.9784130688448075 \tloss:\t 0.0005522402531072579  \tf1 score:\t 0.7295525460755976 \ttime:\t 2.880669355392456\n",
      "fold 2-10 \taccuracy:\t 0.9784130688448075 \tloss:\t 0.00041412744603408296  \tf1 score:\t 0.7295525460755976 \ttime:\t 3.2412571907043457\n",
      "fold 2-11 \taccuracy:\t 0.9784130688448075 \tloss:\t 0.0003191186472616644  \tf1 score:\t 0.7295525460755976 \ttime:\t 2.7632157802581787\n",
      "fold 2-12 \taccuracy:\t 0.9784130688448075 \tloss:\t 0.0002590194627333878  \tf1 score:\t 0.7295525460755976 \ttime:\t 2.8960511684417725\n",
      "fold 2-13 \taccuracy:\t 0.9784130688448075 \tloss:\t 0.00021028404344574286  \tf1 score:\t 0.7295525460755976 \ttime:\t 2.8120369911193848\n",
      "fold 2-14 \taccuracy:\t 0.9784130688448075 \tloss:\t 0.00017518275052007875  \tf1 score:\t 0.729592446044965 \ttime:\t 2.8663220405578613\n",
      "fold 3-0 \taccuracy:\t 0.9521586931155193 \tloss:\t 0.4863973962831432  \tf1 score:\t 0.6563248585546159 \ttime:\t 2.7187693119049072\n",
      "fold 3-1 \taccuracy:\t 0.9702450408401401 \tloss:\t 0.10128164760687429  \tf1 score:\t 0.7558421735244304 \ttime:\t 2.6395084857940674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=474'>475</a>\u001b[0m inputs\u001b[39m=\u001b[39mVariable(inputs)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=476'>477</a>\u001b[0m labels\u001b[39m=\u001b[39mVariable(labels)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=478'>479</a>\u001b[0m outputs,_,_,_ \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=479'>480</a>\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=480'>481</a>\u001b[0m result\u001b[39m.\u001b[39mextend(pred)\n",
      "File \u001b[0;32m~/.conda/envs/sc/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=229'>230</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=230'>231</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=231'>232</a>\u001b[0m     \u001b[39m#x=self.bn0(x)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=232'>233</a>\u001b[0m     TF_residul\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mmatmul(x,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgene_to_TF_transform_matrix)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=234'>235</a>\u001b[0m     TF_derived_from_gene\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgene_to_TF_layer(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=236'>237</a>\u001b[0m     TF_sum\u001b[39m=\u001b[39mTF_residul\u001b[39m+\u001b[39mTF_derived_from_gene\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=237'>238</a>\u001b[0m     \u001b[39m#TF_sum=TF_derived_from_gene\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sc/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=172'>173</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=173'>174</a>\u001b[0m     \u001b[39m#See the autograd section for explanation of what happens here.\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=175'>176</a>\u001b[0m     output\u001b[39m=\u001b[39mLinearFunction\u001b[39m.\u001b[39;49mapply(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=177'>178</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;32m/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39m#change weight to 0 where mask == 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     weight\u001b[39m=\u001b[39mweight\u001b[39m*\u001b[39mmask\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m output\u001b[39m=\u001b[39m\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mmm(weight\u001b[39m.\u001b[39;49mt())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B202.120.45.149/home/wuyou/Projects/scRNA-seq/scGO_Baron_running_time_with_GO_number.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     output\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mbias\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mexpand_as(output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#GO_Net\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split as ts\n",
    "\n",
    "data_rm_sparse=data_rm_sparse\n",
    "\n",
    "data_rm_sparse.index=annotation[\"celltype\"].to_list()\n",
    "\n",
    "###############################################################\n",
    "gene_to_TF_transform_matrix=pickle.load(open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"rb\"))\n",
    "TF_mask=pickle.load(open(\"%s/TF_mask\" %base_dir,\"rb\"))\n",
    "GO_mask=pickle.load(open(\"%s/GO_mask\" %base_dir,\"rb\"))\n",
    "GO_TF_mask=pickle.load(open(\"%s/GO_TF_mask\" %base_dir,\"rb\"))\n",
    "###############################################################\n",
    "#data_annotation = pd.read_csv('data/macparland/GSE115469_CellClusterType.txt', sep=\"\\t\")\n",
    "#index_rename_dict = {key: value for key, value in zip(data_annotation['CellName'], data_annotation['CellType'])}\n",
    "#$data_rm_sparse=data_rm_sparse.rename(index=index_rename_dict)\n",
    "\n",
    "#normalize by row\n",
    "#data_rm_sparse = data_rm_sparse.apply(lambda row: row / np.linalg.norm(row), axis=1)\n",
    "\n",
    "#merge similar cell types\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('Hepatocyte_\\d+', 'Hepatocyte', regex=True)\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('gamma-delta_T_Cells_\\d+', 'gamma-delta_T_Cells', regex=True)\n",
    "\n",
    "\n",
    "#filter low count cells\n",
    "#data_rm_sparse = data_rm_sparse[data_rm_sparse.index != 'Hepatic_Stellate_Cells']\n",
    "\n",
    "\n",
    "#novel_cell_type = ['Plasma_Cells']\n",
    "\n",
    "#data_rm_sparse_novel = data_rm_sparse[data_rm_sparse.index.isin(novel_cell_type)]\n",
    "#data_rm_sparse_rest = data_rm_sparse[~data_rm_sparse.index.isin(novel_cell_type)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes=[]\n",
    "for celltype in data_rm_sparse.index:\n",
    "    if celltype not in classes:\n",
    "        classes.append(celltype)\n",
    "#print(len(classes),classes)\n",
    "\n",
    "\n",
    "label_dict_revese={}\n",
    "label_dict={}\n",
    "for i,celltype in enumerate(classes):\n",
    "    label_dict[celltype]=i\n",
    "    label_dict_revese[i]=celltype\n",
    "label_dict\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def gen_mask(row,col,percent=0.5,num_zeros=None):\n",
    "    if num_zeros is None:\n",
    "        #Total number being masked is 0.5 by default\n",
    "        num_zeros=int((row*col)*percent)\n",
    "    \n",
    "    mask=np.hstack([np.zeros(num_zeros),np.ones(row*col-num_zeros)])\n",
    "    np.random.shuffle(mask)\n",
    "    return mask.reshape(row,col)\n",
    "\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    autograd function which masks it's weights by 'mask'.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Not that both forward and backword are @staticmethod\n",
    "\n",
    "    \n",
    "    #bias, mask is an optional argument\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None, mask=None):\n",
    "        if mask is not None:\n",
    "            #change weight to 0 where mask == 0\n",
    "\n",
    "            weight=weight*mask\n",
    " \n",
    "        output=input.mm(weight.t())\n",
    "\n",
    "        if bias is not None:\n",
    "            output+=bias.unsqueeze(0).expand_as(output)\n",
    "        \n",
    "        ctx.save_for_backward(input, weight, bias, mask)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    #This function has noly a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        input,weight,bias,mask = ctx.saved_tensors\n",
    "        grad_input=grad_weight=grad_bias=grad_mask=None\n",
    "        \n",
    "        #These meeds_input_grad checks are optional and there only to improve efficiency.\n",
    "        #If you want to make your code simpler, you can skip them. Returning gradients for\n",
    "        #inputs that don't require it is not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input=grad_output.mm(weight)\n",
    "        \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight=grad_output.t().mm(input)\n",
    "            \n",
    "            if mask is not None:\n",
    "                \n",
    "                #change grad_weight to 0 where mask == 0\n",
    "                grad_weight=grad_weight*mask\n",
    "\n",
    "        \n",
    "        #if bias is not None and ctx.need_input_grad[2]:\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_bias=grad_output.sum(0).squeeze(0)\n",
    "        \n",
    "        return grad_input,grad_weight,grad_bias,grad_mask\n",
    "    \n",
    "\n",
    "       \n",
    "class CustomizedLinear(nn.Module):\n",
    "    def __init__(self,input_features,output_features, bias=None, mask=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        mask [numpy array]:\n",
    "            The shape is (n_input_fearues,n_output_features).\n",
    "            The elements are 0 or 1 which delcare un-connected or connected.\n",
    "            \n",
    "        bias [bool]:\n",
    "            flg of bias.\n",
    "        \"\"\"\n",
    "        super(CustomizedLinear,self).__init__()\n",
    "        self.input_features=input_features\n",
    "        self.out_features=output_features\n",
    "        \n",
    "        #nn.Parameter is a spetial kind of Tensor, that will get\n",
    "        #automatically registered as Module's parameter once it's assigned\n",
    "        #as an attribute\n",
    "        self.weight=nn.Parameter(torch.Tensor(self.out_features,self.input_features))\n",
    "        \n",
    "        if bias:\n",
    "\n",
    "            self.bias=nn.Parameter(torch.Tensor(self.out_features))\n",
    "        else:\n",
    "            #You should always register all possible parameters, but the\n",
    "            #optinal ones can be None if you want.\n",
    "            self.register_parameter(\"bias\",None)\n",
    "            \n",
    "        #Initialize the above parameters (weight and bias). Important!\n",
    "        self.init_params()\n",
    "        \n",
    "        #mask should be registered after weight and bias\n",
    "        if mask is not None:\n",
    "            mask=torch.tensor(mask,dtype=torch.float).t()\n",
    "            self.mask=nn.Parameter(mask,requires_grad=False)\n",
    "        else:\n",
    "            self.register_parameter(\"mask\",None)\n",
    "\n",
    "        \n",
    "    def init_params(self):\n",
    "        stdv=1./math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv,stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv,stdv)\n",
    "                \n",
    "    def forward(self,input):\n",
    "        #See the autograd section for explanation of what happens here.\n",
    "        \n",
    "        output=LinearFunction.apply(input,self.weight,self.bias,self.mask)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        #(Optional) Set the extra informatioin about this module. You can test\n",
    "        #it by printing an object of this class.\n",
    "        return \"input_features={}, output_features={}, bias={}, mask={}\".format(\n",
    "            self.input_features, self.out_features,\n",
    "            self.bias is not None, self.mask is not None)\n",
    "        \n",
    "        \n",
    "class GO_Net(nn.Module):\n",
    "    def __init__(self,in_size,out_size,ratio=[0.006525,0,0]):\n",
    "        super(GO_Net,self).__init__()\n",
    "\n",
    "        self.gene_number=len(data_rm_sparse.columns)    #6033\n",
    "        self.TF_number=1209\n",
    "        self.GO_number=len(GO_list)\n",
    "        self.class_number=3\n",
    "\n",
    "        self.gene_to_TF_transform_matrix=torch.tensor(gene_to_TF_transform_matrix,dtype=torch.float32)\n",
    "    \n",
    "        \n",
    "        self.bn0=nn.BatchNorm1d(self.gene_number)\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=gen_mask(3443,2290,ratio[0]))  \n",
    "        #self.fc1=CustomizedLinear(in_size,1946,mask=gen_mask(2903,1946,ratio[0]))        \n",
    "        self.fc1=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        self.gene_to_GO_layer=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=np.ones((3443,2290)))\n",
    "    \n",
    "        self.bn1=nn.BatchNorm1d(self.GO_number)\n",
    "                \n",
    "        self.fc2=CustomizedLinear(self.GO_number,out_size,mask=gen_mask(self.GO_number,out_size,ratio[1]))\n",
    "        self.bn2=nn.BatchNorm1d(out_size)\n",
    "\n",
    "        self.gene_to_TF_layer=CustomizedLinear(self.gene_number,self.TF_number,mask=TF_mask)\n",
    "        self.TF_to_GO_layer=CustomizedLinear(self.TF_number,self.GO_number,mask=GO_TF_mask)\n",
    "        \n",
    "        self.fc3=CustomizedLinear(100,100,mask=gen_mask(100,100,ratio[1]))\n",
    "\n",
    "        self.fc4=CustomizedLinear(100,out_size,mask=gen_mask(100,out_size,ratio[1]))\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        self.leaky_relu=nn.LeakyReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.uniform_(module.weight,a=0,b=1)\n",
    "            elif isinstance(module,(nn.BatchNorm1d,nn.GroupNorm)):\n",
    "                nn.init.constant_(module.weight,1)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "                        \n",
    "    def forward(self,x):\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        TF_residul=torch.matmul(x,self.gene_to_TF_transform_matrix)\n",
    "\n",
    "        TF_derived_from_gene=self.gene_to_TF_layer(x)\n",
    "\n",
    "        TF_sum=TF_residul+TF_derived_from_gene\n",
    "        #TF_sum=TF_derived_from_gene\n",
    "\n",
    "        GO_derived_from_TF=self.TF_to_GO_layer(TF_sum)\n",
    "\n",
    "        GO_derived_from_gene=self.gene_to_GO_layer(x)\n",
    "\n",
    "        GO_sum=GO_derived_from_TF+GO_derived_from_gene\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        #x=self.fc1(x)\n",
    "        #x=self.bn1(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.dropout(x)\n",
    "        GO_sum=self.leaky_relu(GO_sum)\n",
    "\n",
    "        #x=torch.tanh(x) \n",
    "        #print(161,self.fc1.weight)\n",
    "        x=self.fc2(GO_sum)\n",
    "        #x=self.bn2(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc3(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc4(x)\n",
    " \n",
    "        return x,GO_sum,TF_derived_from_gene,GO_derived_from_TF\n",
    "\n",
    "\"\"\"\n",
    "class Reconstraction(nn.Module):\n",
    "    def __init__(self,in_size,out_size):\n",
    "        super(Reconstraction,self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_size, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 1000),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(1000, out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\"\"\"     \n",
    "\n",
    "\n",
    " \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.x[index]\n",
    "        label = self.y[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def accuracy_score(y_test,y_pred):\n",
    "    t=0\n",
    "    f=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==y_pred[i]:\n",
    "            t+=1\n",
    "        else:\n",
    "            f+=1\n",
    "    return(t/(t+f))\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def make_weights_for_balanced_classes(dataset, nclasses):\n",
    "    count = [0] * nclasses\n",
    "    for item in dataset:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    N = float(sum(count))\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(dataset)\n",
    "    for idx, val in enumerate(dataset):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight\n",
    "\n",
    "\n",
    "class CustomWeightedRandomSampler(WeightedRandomSampler):\n",
    "    \"\"\"WeightedRandomSampler except allows for more than 2^24 samples to be sampled\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_tensor = np.random.choice(range(0, len(self.weights)),\n",
    "                                       size=self.num_samples,\n",
    "                                       p=self.weights.numpy() / torch.sum(self.weights).numpy(),\n",
    "                                       replace=self.replacement)\n",
    "        rand_tensor = torch.from_numpy(rand_tensor)\n",
    "        return iter(rand_tensor.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#training\n",
    "input_size = len(data_rm_sparse.columns)\n",
    "output_size = len(classes)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "#reconstraction_optimizer = optim.Adam(reconstraction_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#reconstraction_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "data_train_x=data_rm_sparse\n",
    "data_train_y=data_rm_sparse.index\n",
    "\n",
    "\n",
    "#5-fold cross validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Generate 5-fold cross-validation indices\n",
    "kf = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_indices = list(kf.split(data))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for fold, (train_indices, test_indices) in enumerate(fold_indices, start=1):\n",
    "\n",
    "    #define model and optimizer\n",
    "    model = GO_Net(input_size, output_size,ratio=[0,0,0])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # Subset data and annotation based on indices\n",
    "    x_train = data_train_x.iloc[train_indices].to_numpy()\n",
    "    y_train = data_train_y[train_indices,]\n",
    "    \n",
    "    x_test = data_train_x.iloc[test_indices].to_numpy()\n",
    "    y_test = data_train_y[test_indices,]\n",
    "\n",
    "    # Continue with your operations on data_train, anno_train, data_test, and anno_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #x_train,x_test,y_train,y_test = ts(data_train_x.to_numpy(),data_train_y.to_numpy(),test_size=0.2,random_state=1, shuffle=True)\n",
    "\n",
    "    #x_train=x_train[0:400]\n",
    "    #y_train=y_train[0:400]\n",
    "\n",
    "    #label_dict={25:0,26:1,27:2,33:3,34:4}\n",
    "    y_train_relabeled=[label_dict[label] for label in y_train]\n",
    "    y_test_relabeled=[label_dict[label] for label in y_test]\n",
    "\n",
    "\n",
    "    #train_size=20000\n",
    "\n",
    "    #x_train=x_train[0:train_size]\n",
    "    #y_train_relabeled=y_train_relabeled[0:train_size]\n",
    "\n",
    "    train_data=MyDataset(x_train,y_train_relabeled)\n",
    "\n",
    "\n",
    "\n",
    "    #for unbalanced data\n",
    "    \"\"\"\n",
    "    weights=make_weights_for_balanced_classes(train_data,len(classes))\n",
    "    weights = torch.DoubleTensor(weights)\n",
    "    sampler = CustomWeightedRandomSampler(weights, len(weights))        #sampler for imbalanced classes\n",
    "    \"\"\"\n",
    "\n",
    "    #train_loader=DataLoader(train_data, batch_size=60, sampler=sampler)\n",
    "    train_loader=DataLoader(train_data, batch_size=60, shuffle=True)\n",
    "\n",
    "    num_epochs=15\n",
    "    # è®­ç»ƒæ¨¡åž‹\n",
    "    for epoch in range(num_epochs):\n",
    "        tic=time.time()\n",
    "        running_loss = 0.0\n",
    "        reconstraction_running_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            inputs, labels = batch\n",
    "            #print(labels)\n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            # å°†æ¢¯åº¦ç¼“å­˜æ¸…é›¶\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # å‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±å’Œåå‘ä¼ æ’­\n",
    "            outputs,_,_,_ = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #reconstraction_input=reconstraction_model(outputs)\n",
    "            #reconstraction_loss = reconstraction_criterion(reconstraction_input, inputs)\n",
    "\n",
    "            #reconstraction_optimizer.zero_grad()\n",
    "\n",
    "            #combined_loss=loss+reconstraction_loss\n",
    "            #combined_loss.backward()\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            #reconstraction_optimizer.step()\n",
    "\n",
    "\n",
    "            #reconstraction_running_loss += reconstraction_loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 40 == 0:\n",
    "                pass\n",
    "                #print(i)\n",
    "                #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            \n",
    "            if i>400:\n",
    "                break\n",
    "\n",
    "        test_data=MyDataset(x_test,y_test_relabeled)\n",
    "        test_loader=DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "        result=[]\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            \n",
    "            outputs,_,_,_ = model(inputs)\n",
    "            pred = list(torch.max(outputs, 1)[1].numpy())\n",
    "            result.extend(pred)\n",
    "            #print(pred,labels)\n",
    "            if i>100:\n",
    "                break\n",
    "        accuracy = accuracy_score(y_test_relabeled[0:len(result)],result)\n",
    "        f1_score = calculate_multiclass_f1_score(y_test_relabeled[0:len(result)],result)\n",
    "                #########\n",
    "\n",
    "        toc=time.time()\n",
    "        print(\"fold %s-%s\" %(fold,epoch),\"\\taccuracy:\\t\",accuracy,\"\\tloss:\\t\",running_loss / len(train_loader),\" \\tf1 score:\\t\",f1_score, \"\\ttime:\\t\", toc-tic )\n",
    "        \n",
    "        #save model\n",
    "        #pickle.dump(model,open(\"model/GO_heart.model\",\"wb\"))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GO_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Probabilities  Proportion\n",
      "0           200        2.61\n",
      "1           400        2.88\n",
      "2           600        3.10\n",
      "3           800        3.28\n",
      "4          1000        3.27\n",
      "5          1200        3.30\n",
      "6          1400        3.45\n",
      "7          1600        3.89\n",
      "8          1800        3.99\n",
      "9          1946        4.13\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 354x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 180,
       "width": 354
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyou/.conda/envs/sc/lib/python3.8/site-packages/plotnine/ggplot.py:587: PlotnineWarning: Saving 3.54 x 1.8 in image.\n",
      "/home/wuyou/.conda/envs/sc/lib/python3.8/site-packages/plotnine/ggplot.py:588: PlotnineWarning: Filename: figure/running_time_with_GO_number.pdf\n"
     ]
    }
   ],
   "source": [
    "#probabilities cutoff barplot\n",
    "X=[\"200\",\"400\",\"600\",\"800\",\"1000\",\"1200\",\"1400\",\"1600\",\"1800\",\"1946\"]\n",
    "Y=[2.61, 2.88, 3.10, 3.28, 3.27, 3.30, 3.45,3.89,  3.99, 4.13]\n",
    "\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "\n",
    "black = '#222222'\n",
    "gray = '#666666'\n",
    "red = '#FF3333'\n",
    "green = '#66CC00'\n",
    "blue = '#3333FF'\n",
    "purple = '#9933FF'\n",
    "orange = '#FF8000'\n",
    "yellow = '#FFFF33'\n",
    "c1=\"#F8766D\"\n",
    "c2=\"#00BA38\" \n",
    "c3=\"#619CFF\"\n",
    "\n",
    "data=pd.DataFrame(dict(Probabilities=X,Proportion=Y)) \n",
    "data['Probabilities'] = pd.Categorical(data.Probabilities, categories=pd.unique(data.Probabilities))  #reorder legend\n",
    "\n",
    "print(data)\n",
    "p1 = (ggplot()\n",
    "        +geom_bar(data,aes(x=\"Probabilities\",y = \"Proportion\"),stat=\"identity\",width=0.6,fill=c1,alpha=0.8)\n",
    "        \n",
    "        +theme(panel_background=element_rect(fill=gray, alpha=0),\n",
    "            panel_grid_major=element_line(size=0.3, alpha=0,color=black),\n",
    "            panel_grid_minor=element_line(size=0.3, alpha=0,color=black),\n",
    "            panel_border=element_rect(color=black, size=1),\n",
    "            axis_text=element_text(size=6,family=\"Arial\",color=\"black\"),\n",
    "            axis_title_x=element_text(size=6,family=\"Arial\",color=\"black\"),\n",
    "            axis_title_y=element_text(size=6,family=\"Arial\",color=\"black\"),\n",
    "            axis_text_x=element_text(rotation=0, hjust=0.5),\n",
    "            figure_size=[3.54,1.8],\n",
    "            legend_title = element_text(size=6), #change legend title font size\n",
    "            legend_text = element_text(size=6),\n",
    "            legend_background=element_rect(size=0.5,alpha=0),\n",
    "            legend_position=(0.60,0.4),\n",
    "            legend_key_size=4) #change legend text font size\n",
    "        +labs(x = \"GO number\", y =\"Running time per epoch (s)\")\n",
    "        +guides(color = guide_legend(title = \"Probability cutoff\"))\n",
    "        + coord_cartesian(ylim=(0, 4.5))\n",
    ")\n",
    "print(p1)\n",
    "p1.save('figure/running_time_with_GO_number.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
